{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9bff53e",
   "metadata": {},
   "source": [
    "# Basic RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60175c42-62b9-4eca-a1e1-2a5846d1fc9a",
   "metadata": {},
   "source": [
    "In this notebook we will look into building an basic RAG pipeline with LlamaIndex. It has following 2 sections.\n",
    "\n",
    "1. Understanding Retrieval Augmented Generation (RAG).\n",
    "2. Building basic RAG with LlamaIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9fd12c-8c5e-452d-bcf9-01e1c0eb6fbe",
   "metadata": {},
   "source": [
    "**Retrieval Augmented Generation (RAG)**\n",
    "\n",
    "LLMs are trained on vast datasets, but these will not include your specific data. Retrieval-Augmented Generation (RAG) addresses this by dynamically incorporating your data during the generation process. This is done not by altering the training data of LLMs, but by allowing the model to access and utilize your data in real-time to provide more tailored and contextually relevant responses.\n",
    "\n",
    "In RAG, your data is loaded and and prepared for queries or “indexed”. User queries act on the index, which filters your data down to the most relevant context. This context and your query then go to the LLM along with a prompt, and the LLM provides a response.\n",
    "\n",
    "Even if what you’re building is a chatbot or an agent, you’ll want to know RAG techniques for getting data into your application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd18319-426e-4bdc-892b-49c38750976f",
   "metadata": {},
   "source": [
    "![RAG Overview](../data/llamaindex_rag_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa39061c-f0fa-443b-8b76-4a1fc0ce9e90",
   "metadata": {},
   "source": [
    "**Stages within RAG**\n",
    "\n",
    "There are five key stages within RAG, which in turn will be a part of any larger application you build. These are:\n",
    "\n",
    "**Loading:** this refers to getting your data from where it lives – whether it’s text files, PDFs, another website, a database, or an API – into your pipeline. LlamaHub provides hundreds of connectors to choose from.\n",
    "\n",
    "**Indexing:** this means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\n",
    "\n",
    "**Storing:** Once your data is indexed, you will want to store your index, along with any other metadata, to avoid the need to re-index it.\n",
    "\n",
    "**Querying:** for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.\n",
    "\n",
    "**Evaluation:** a critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are. However, this part is not covered in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfa1339-3e31-46cd-9ba4-a58165feeda6",
   "metadata": {},
   "source": [
    "## Build RAG system.\n",
    "\n",
    "Now that we have understood the significance of RAG system, let's build a simple basci RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224bad36-4af8-496b-a07d-7e172616f059",
   "metadata": {},
   "source": [
    "Set Your OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49305594-8986-44e5-8d7d-27f76df9981b",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henry/anaconda3/envs/NLP/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Context Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Context Relevance, input response will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "✅ In Groundedness, input source will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import common.utils\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = common.utils.get_openai_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a850ab0-6d83-493a-a829-956e146c64e1",
   "metadata": {},
   "source": [
    "#### Load Data and Build Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "101152e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"../data/Henry.txt\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d7d0857-b9d1-4feb-8243-bfd2f4953acd",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> \n",
      "\n",
      "1 \n",
      "\n",
      "<class 'llama_index.schema.Document'>\n",
      "Doc ID: 9e4ed240-1cd5-4c00-bd39-57392ec1ffcb\n",
      "Text: History   Henry, with his striking features and undeniable\n",
      "charm, has captivated the hearts of many in Hong Kong, earning him the\n",
      "title of the most handsome boy in the city. His chiseled jawline,\n",
      "expressive eyes, and perfectly styled hair make heads turn wherever he\n",
      "goes. Beyond his physical appearance, Henry possesses an innate grace\n",
      "and confid...\n"
     ]
    }
   ],
   "source": [
    "print(type(documents), \"\\n\")\n",
    "print(len(documents), \"\\n\")\n",
    "print(type(documents[0]))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3123d3d",
   "metadata": {},
   "source": [
    "concatenate all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4abc806-64f5-46bb-8c9f-6469ecb18d20",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31bb6ea-07bb-43c4-b584-da3ac3522a0a",
   "metadata": {},
   "source": [
    "build service context and establish index by using specifc embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7620d382-50d8-4108-80c0-2e582ebeaca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# Define an LLM\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Build index with a chunk_size of 64\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=64,chunk_overlap=2)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f9c68c-e188-484e-b685-55047ea7b45f",
   "metadata": {},
   "source": [
    "Build a QueryEngine and start querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae52a26c-7d0c-44df-8043-4c7f19f794b9",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b0d5b6e-cc2e-4648-b28c-5fa25a97d175",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Henry is the pretty boy in Hong Kong.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"Who is the pretty boy in Hong Kong?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57c76e6d-0c7e-4244-b3ec-fd6c220ef04d",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Henry是香港最帅的男孩。\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"香港谁最帅?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4511a029-a5b0-433d-88dd-7bc9480ee31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Henri is the beautiful person in Hong Kong.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"Who is the beautiful person in Hong Kong?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706bedda-6483-416a-aa78-40122316c954",
   "metadata": {},
   "source": [
    "By default it retrieves `two` similar nodes/ chunks. You can modify that in `vector_index.as_query_engine(similarity_top_k=k)`.\n",
    "\n",
    "**Let's check the text in each of these retrieved nodes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95dc83e5-97be-4fa8-b97f-26783bba251d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's no wonder that he has become an icon of attractiveness in Hong Kong, leaving a lasting impression on everyone fortunate enough to encounter him.\\n\\nHenri, with her radiant presence and captivating allure, is hailed as the most beautiful girl in Hong Kong.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First retrieved node\n",
    "response.source_nodes[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7980292a-e8f7-4c8b-8d9b-c3072500fb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Whether she is engaged in a conversation or simply walking down the streets of Hong Kong, Henri's beauty is captivating, leaving an indelible impression on those fortunate enough to cross her path.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second retrieved node\n",
    "response.source_nodes[1].get_text()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
